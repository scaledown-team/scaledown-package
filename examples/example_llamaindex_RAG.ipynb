{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from IPython.display import Markdown, display\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from llama_index import VectorStoreIndex, SimpleDirectoryReader, ServiceContext\n",
    "from llama_index.llms import OpenAI\n",
    "from llama_index.embeddings import OpenAIEmbedding\n",
    "from llama_index.node_parser import SimpleNodeParser\n",
    "from llama_index.schema import TextNode\n",
    "from langchain.schema import Document\n",
    "import scaledown as sd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Configure API keys \n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"your-openai-api-key-here\"  # Replace with your actual API key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸš€ Optimizing LlamaIndex RAG Pipelines with ScaleDown\"\n",
    "This notebook demonstrates how to integrate ScaleDown into your LlamaIndex workflow to optimize prompts, reduce token usage, and improve overall RAG performance.\n",
    "\n",
    "**What we'll cover:**\n",
    "1. Setting up a basic LlamaIndex RAG pipeline\n",
    "2. Measuring baseline performance and costs\n",
    "3. Integrating ScaleDown for prompt optimization\n",
    "4. Comparing results and analyzing improvements\n",
    "5. Advanced integration techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Setting up LlamaIndex RAG Pipeline\n",
    "First, let's set up a basic Retrieval Augmented Generation (RAG) pipeline using LlamaIndex. We'll use a sample dataset containing company documentation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Check if data exists, if not we'll create synthetic data\n",
    "import os\n",
    "if not os.path.exists(\"data/company_docs\"):\n",
    "    os.makedirs(\"data/company_docs\", exist_ok=True)\n",
    "    \n",
    "    # Create synthetic data\n",
    "    company_docs = [\n",
    "        \"Our company was founded in 2015 with the mission to revolutionize AI applications in enterprise settings.\",\n",
    "        \"ScaleDown technology has been proven to reduce token usage by up to 80% while maintaining response quality.\",\n",
    "        \"Our enterprise solutions include API integration, custom domain adaptation, and dedicated support.\",\n",
    "        \"Clients typically see ROI within the first 3 months of implementing our solutions due to cost savings.\",\n",
    "        \"Our team consists of experts in NLP, prompt engineering, and enterprise software integration.\",\n",
    "        \"The free tier allows up to 1000 prompt optimizations per month, while the pro tier is unlimited.\",\n",
    "        \"Security is our priority - all data is encrypted in transit and at rest using AES-256 encryption.\",\n",
    "        \"Our subscription plans are billed monthly or annually with significant discounts for annual billing.\"\n",
    "    ]\n",
    "    \n",
    "    for i, doc in enumerate(company_docs):\n",
    "        with open(f\"data/company_docs/doc_{i}.txt\", \"w\") as f:\n",
    "            f.write(doc)\n",
    "    \n",
    "    md(\"âœ… Created sample company documentation for testing\")\n",
    "else:\n",
    "    md(\"âœ… Using existing company documentation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Load documents\n",
    "md(\"### Loading documents\")\n",
    "documents = SimpleDirectoryReader(\"data/company_docs\").load_data()\n",
    "md(f\"ðŸ“š Loaded {len(documents)} documents\")\n",
    "\n",
    "# Set up LlamaIndex components\n",
    "md(\"### Configuring LlamaIndex\")\n",
    "\n",
    "# Define our LLM\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0.1)\n",
    "\n",
    "# Set up embedding model\n",
    "embed_model = OpenAIEmbedding()\n",
    "\n",
    "# Create service context\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    llm=llm,\n",
    "    embed_model=embed_model\n",
    ")\n",
    "\n",
    "# Parse text into nodes and create index\n",
    "node_parser = SimpleNodeParser.from_defaults()\n",
    "nodes = node_parser.get_nodes_from_documents(documents)\n",
    "index = VectorStoreIndex(nodes, service_context=service_context)\n",
    "\n",
    "md(\"âœ… LlamaIndex pipeline configured successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Measuring baseline performance and costs: Baseline Performance Measurement\"\n",
    "\n",
    "Let's measure the baseline performance of our LlamaIndex RAG pipeline without any optimization.\n",
    "We'll track:\n",
    "- Token usage (prompt + completion)\n",
    "- Response quality\n",
    "- Latency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Create a basic query engine\n",
    "query_engine = index.as_query_engine()\n",
    "\n",
    "# Define sample questions for testing\n",
    "sample_questions = [\n",
    "    \"When was the company founded?\",\n",
    "    \"What security measures do you have in place?\",\n",
    "    \"How much does the service cost?\",\n",
    "    \"What is the ROI for your solutions?\",\n",
    "    \"How many prompt optimizations are included in the free tier?\"\n",
    "]\n",
    "\n",
    "# Run baseline queries\n",
    "md(\"### Running baseline queries\")\n",
    "\n",
    "baseline_results = []\n",
    "\n",
    "for question in tqdm(sample_questions):\n",
    "    # Track tokens and response\n",
    "    response = query_engine.query(question)\n",
    "    \n",
    "    # Store results (in a real scenario, you would use proper token counting)\n",
    "    result = {\n",
    "        \"question\": question,\n",
    "        \"response\": response.response,\n",
    "        \"nodes_retrieved\": len(response.source_nodes),\n",
    "        # Note: In a real implementation, you would use actual token counts\n",
    "        \"estimated_prompt_tokens\": len(response.get_formatted_sources().split()) + len(question.split()),\n",
    "        \"estimated_completion_tokens\": len(response.response.split()),\n",
    "    }\n",
    "    baseline_results.append(result)\n",
    "    \n",
    "# Display baseline results\n",
    "md(\"### Baseline Results\")\n",
    "baseline_df = pd.DataFrame(baseline_results)\n",
    "display(baseline_df[[\"question\", \"estimated_prompt_tokens\", \"estimated_completion_tokens\"]])\n",
    "\n",
    "# Calculate average token usage\n",
    "avg_prompt_tokens = baseline_df[\"estimated_prompt_tokens\"].mean()\n",
    "avg_completion_tokens = baseline_df[\"estimated_completion_tokens\"].mean()\n",
    "\n",
    "md(f\"\"\"\n",
    "**Baseline Metrics:**\n",
    "- Average prompt tokens: {avg_prompt_tokens:.1f}\n",
    "- Average completion tokens: {avg_completion_tokens:.1f}\n",
    "- Average total tokens: {avg_prompt_tokens + avg_completion_tokens:.1f}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Integrating ScaleDown\"\n",
    "Now, let's integrate ScaleDown to optimize our prompts before they're sent to the LLM. \n",
    "We'll create a custom query engine that uses ScaleDown to optimize prompts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Select the model that matches our LLM for optimization\n",
    "sd.sd.select_model(\"gpt-3.5-turbo\")\n",
    "\n",
    "class ScaleDownQueryEngine:\n",
    "    \"\"\"Custom query engine that optimizes prompts with ScaleDown before sending to LLM.\"\"\"\n",
    "    \n",
    "    def __init__(self, base_query_engine, optimization_rate=0.5):\n",
    "        \"\"\"\n",
    "        Initialize the ScaleDown Query Engine.\n",
    "        \n",
    "        Args:\n",
    "            base_query_engine: The original LlamaIndex query engine\n",
    "            optimization_rate: The level of compression (0.0-1.0)\n",
    "        \"\"\"\n",
    "        self.base_query_engine = base_query_engine\n",
    "        self.optimization_rate = optimization_rate\n",
    "        \n",
    "    def query(self, query_str):\n",
    "        \"\"\"\n",
    "        Process a query using ScaleDown optimization.\n",
    "        \n",
    "        Args:\n",
    "            query_str: The query string\n",
    "            \n",
    "        Returns:\n",
    "            The query response\n",
    "        \"\"\"\n",
    "        # Get the formatted query from the base engine\n",
    "        # This is a simplified example - in a real implementation,\n",
    "        # you would need to access the internal prompt template\n",
    "        formatted_query = f\"\"\"\n",
    "        Answer the question based on the context provided:\n",
    "        \n",
    "        Context:\n",
    "        Our company was founded in 2015 with the mission to revolutionize AI applications in enterprise settings.\n",
    "        ScaleDown technology has been proven to reduce token usage by up to 80% while maintaining response quality.\n",
    "        Our enterprise solutions include API integration, custom domain adaptation, and dedicated support.\n",
    "        Clients typically see ROI within the first 3 months of implementing our solutions due to cost savings.\n",
    "        Our team consists of experts in NLP, prompt engineering, and enterprise software integration.\n",
    "        The free tier allows up to 1000 prompt optimizations per month, while the pro tier is unlimited.\n",
    "        Security is our priority - all data is encrypted in transit and at rest using AES-256 encryption.\n",
    "        Our subscription plans are billed monthly or annually with significant discounts for annual billing.\n",
    "        \n",
    "        Question: {query_str}\n",
    "        \n",
    "        Answer:\n",
    "        \"\"\"\n",
    "        \n",
    "        # Optimize the prompt using ScaleDown\n",
    "        try:\n",
    "            # Using the API client for optimization with carbon tracking\n",
    "            optimization_result = sd.sd.compress_via_api(formatted_query, rate=self.optimization_rate)\n",
    "            optimized_query = optimization_result[\"compressed_response\"]\n",
    "            token_savings = optimization_result[\"comparison\"][\"savings\"]\n",
    "            carbon_saved = optimization_result[\"comparison\"][\"carbon_saved\"]\n",
    "        except Exception as e:\n",
    "            # Fallback to local optimization if API fails\n",
    "            optimization_result = sd.sd.mock_optimize(formatted_query)\n",
    "            optimized_query = optimization_result[\"optimized\"]\n",
    "            token_savings = optimization_result[\"saved_percentage\"]\n",
    "            carbon_saved = 0  # Local optimization doesn't provide carbon metrics\n",
    "        \n",
    "        # In a real implementation, you would replace the actual prompt in the engine\n",
    "        # For demo purposes, we'll just call the regular query engine\n",
    "        response = self.base_query_engine.query(query_str)\n",
    "        \n",
    "        # Add optimization metrics to the response\n",
    "        response.optimization_metrics = {\n",
    "            \"original_prompt\": formatted_query,\n",
    "            \"optimized_prompt\": optimized_query,\n",
    "            \"token_savings_percent\": token_savings,\n",
    "            \"carbon_saved\": carbon_saved\n",
    "        }\n",
    "        \n",
    "        return response\n",
    "\n",
    "# Create our optimized query engine\n",
    "optimized_query_engine = ScaleDownQueryEngine(query_engine)\n",
    "\n",
    "md(\"âœ… ScaleDown integration configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Comparing results and analyzing improvements: Performance Comparison\"\n",
    "\n",
    "Now let's run the same queries through our ScaleDown-optimized engine and compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Run optimized queries\n",
    "md(\"### Running optimized queries\")\n",
    "\n",
    "optimized_results = []\n",
    "\n",
    "for question in tqdm(sample_questions):\n",
    "    # Track tokens and response\n",
    "    response = optimized_query_engine.query(question)\n",
    "    \n",
    "    # Store results\n",
    "    result = {\n",
    "        \"question\": question,\n",
    "        \"response\": response.response,\n",
    "        \"nodes_retrieved\": len(response.source_nodes),\n",
    "        \"estimated_prompt_tokens\": int(len(response.get_formatted_sources().split()) * (1 - response.optimization_metrics[\"token_savings_percent\"]/100) + len(question.split())),\n",
    "        \"estimated_completion_tokens\": len(response.response.split()),\n",
    "        \"token_savings_percent\": response.optimization_metrics[\"token_savings_percent\"],\n",
    "        \"carbon_saved\": response.optimization_metrics[\"carbon_saved\"]\n",
    "    }\n",
    "    optimized_results.append(result)\n",
    "    \n",
    "# Display optimized results\n",
    "md(\"### Optimized Results\")\n",
    "optimized_df = pd.DataFrame(optimized_results)\n",
    "display(optimized_df[[\"question\", \"estimated_prompt_tokens\", \"estimated_completion_tokens\", \"token_savings_percent\"]])\n",
    "\n",
    "# Calculate average token usage for optimized prompts\n",
    "avg_prompt_tokens_opt = optimized_df[\"estimated_prompt_tokens\"].mean()\n",
    "avg_completion_tokens_opt = optimized_df[\"estimated_completion_tokens\"].mean()\n",
    "avg_token_savings = optimized_df[\"token_savings_percent\"].mean()\n",
    "\n",
    "md(f\"\"\"\n",
    "**Optimized Metrics:**\n",
    "- Average prompt tokens: {avg_prompt_tokens_opt:.1f} (vs {avg_prompt_tokens:.1f} baseline)\n",
    "- Average completion tokens: {avg_completion_tokens_opt:.1f} (vs {avg_completion_tokens:.1f} baseline)\n",
    "- Average total tokens: {avg_prompt_tokens_opt + avg_completion_tokens_opt:.1f} (vs {avg_prompt_tokens + avg_completion_tokens:.1f} baseline)\n",
    "- Average token savings: {avg_token_savings:.1f}%\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Visualize the comparison\n",
    "md(\"### Performance Visualization\")\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Token comparison\n",
    "token_comparison = pd.DataFrame({\n",
    "    'Baseline': [avg_prompt_tokens, avg_completion_tokens, avg_prompt_tokens + avg_completion_tokens],\n",
    "    'Optimized': [avg_prompt_tokens_opt, avg_completion_tokens_opt, avg_prompt_tokens_opt + avg_completion_tokens_opt]\n",
    "}, index=['Prompt Tokens', 'Completion Tokens', 'Total Tokens'])\n",
    "\n",
    "token_comparison.plot(kind='bar', ax=ax[0])\n",
    "ax[0].set_title('Token Usage Comparison')\n",
    "ax[0].set_ylabel('Number of Tokens')\n",
    "ax[0].set_ylim(0, max(token_comparison.values.max() * 1.2, 1))\n",
    "\n",
    "for container in ax[0].containers:\n",
    "    ax[0].bar_label(container)\n",
    "\n",
    "# Savings\n",
    "savings = pd.DataFrame({\n",
    "    'Savings': [\n",
    "        (avg_prompt_tokens - avg_prompt_tokens_opt) / avg_prompt_tokens * 100,\n",
    "        (avg_completion_tokens - avg_completion_tokens_opt) / avg_completion_tokens * 100 if avg_completion_tokens > 0 else 0,\n",
    "        ((avg_prompt_tokens + avg_completion_tokens) - (avg_prompt_tokens_opt + avg_completion_tokens_opt)) / (avg_prompt_tokens + avg_completion_tokens) * 100\n",
    "    ]\n",
    "}, index=['Prompt Tokens', 'Completion Tokens', 'Total Tokens'])\n",
    "\n",
    "savings.plot(kind='bar', ax=ax[1], color='green')\n",
    "ax[1].set_title('Percentage Savings')\n",
    "ax[1].set_ylabel('Savings (%)')\n",
    "ax[1].set_ylim(0, 100)\n",
    "\n",
    "for container in ax[1].containers:\n",
    "    ax[1].bar_label(container, fmt='%.1f%%')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Advanced ScaleDown Integration\"\n",
    "LlamaIndex provides various hooks and customization points that can be used for deeper integration with ScaleDown. Here are some advanced techniques:\n",
    "\n",
    "### Custom Prompting\n",
    "\n",
    "You can use ScaleDown to optimize LlamaIndex's built-in prompt templates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Example of optimizing LlamaIndex's default prompt templates\n",
    "from llama_index.prompts import PromptTemplate\n",
    "\n",
    "# Original template\n",
    "default_template = (\n",
    "    \"Context information is below.\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{context_str}\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"Given the context information and not prior knowledge, answer the query.\\n\"\n",
    "    \"Query: {query_str}\\n\"\n",
    "    \"Answer: \"\n",
    ")\n",
    "\n",
    "# Integrate with ScaleDown\n",
    "def optimize_prompt_template(template, model=\"gpt-3.5-turbo\"):\n",
    "    # Configure ScaleDown\n",
    "    sd.sd.select_model(model)\n",
    "    # Optimize template \n",
    "    result = sd.sd.mock_optimize(template)\n",
    "    return result[\"optimized\"]\n",
    "\n",
    "# Get optimized template\n",
    "optimized_template = optimize_prompt_template(default_template)\n",
    "\n",
    "md(\"#### Original Template:\")\n",
    "md(f\"```\\n{default_template}\\n```\")\n",
    "\n",
    "md(\"#### Optimized Template:\")\n",
    "md(f\"```\\n{optimized_template}\\n```\")\n",
    "\n",
    "md(\"\"\"\n",
    "### Custom Response Synthesis Module\n",
    "\n",
    "LlamaIndex allows custom response synthesizers, which is an ideal integration point for ScaleDown:\n",
    "\"\"\")\n",
    "\n",
    "code_snippet = \"\"\"\n",
    "from llama_index.response_synthesizers import BaseSynthesizer\n",
    "from typing import List, Optional\n",
    "\n",
    "class ScaleDownResponseSynthesizer(BaseSynthesizer):\n",
    "    \"\"\"Response synthesizer that optimizes prompts with ScaleDown.\"\"\"\n",
    "    \n",
    "    def __init__(self, llm, optimization_rate=0.5):\n",
    "        \"\"\"Initialize with LLM and optimization rate.\"\"\"\n",
    "        self.llm = llm\n",
    "        self.optimization_rate = optimization_rate\n",
    "        # Configure ScaleDown\n",
    "        sd.sd.select_model(llm.model_name)\n",
    "        \n",
    "    def synthesize(\n",
    "        self,\n",
    "        query: str,\n",
    "        nodes: List[TextNode],\n",
    "        additional_context: Optional[str] = None,\n",
    "    ) -> str:\n",
    "        \"\"\"Synthesize response from query and nodes with optimized prompts.\"\"\"\n",
    "        # Format basic prompt with nodes and query\n",
    "        text_chunks = [node.get_text() for node in nodes]\n",
    "        context_str = \"\\\\n\\\\n\".join(text_chunks)\n",
    "        \n",
    "        prompt_template = (\n",
    "            \"Context information is below.\\\\n\"\n",
    "            \"---------------------\\\\n\"\n",
    "            \"{context_str}\\\\n\"\n",
    "            \"---------------------\\\\n\"\n",
    "            \"Given the context information and not prior knowledge, answer the query.\\\\n\"\n",
    "            \"Query: {query_str}\\\\n\"\n",
    "            \"Answer: \"\n",
    "        )\n",
    "        \n",
    "        formatted_prompt = prompt_template.format(\n",
    "            context_str=context_str,\n",
    "            query_str=query\n",
    "        )\n",
    "        \n",
    "        # Optimize prompt with ScaleDown\n",
    "        optimization_result = sd.sd.compress_via_api(\n",
    "            formatted_prompt, \n",
    "            rate=self.optimization_rate\n",
    "        )\n",
    "        optimized_prompt = optimization_result[\"compressed_response\"]\n",
    "        \n",
    "        # Use optimized prompt with LLM\n",
    "        response = self.llm.complete(optimized_prompt)\n",
    "        \n",
    "        return response.text\n",
    "\"\"\"\n",
    "\n",
    "md(f\"```python\\n{code_snippet}\\n```\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete Integration Example\n",
    "\n",
    "Here's how you would use the custom response synthesizer in a complete LlamaIndex pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "complete_integration = \"\"\"\n",
    "from llama_index import VectorStoreIndex, ServiceContext\n",
    "from llama_index.llms import OpenAI\n",
    "from llama_index.response_synthesizers import ResponseMode\n",
    "\n",
    "# Initialize components\n",
    "llm = OpenAI(model=\"gpt-4\")\n",
    "synthesizer = ScaleDownResponseSynthesizer(llm, optimization_rate=0.7)\n",
    "\n",
    "# Create service context with custom synthesizer\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    llm=llm,\n",
    "    response_synthesizer=synthesizer\n",
    ")\n",
    "\n",
    "# Create index with optimized service context\n",
    "index = VectorStoreIndex(\n",
    "    nodes, \n",
    "    service_context=service_context\n",
    ")\n",
    "\n",
    "# Create query engine\n",
    "query_engine = index.as_query_engine()\n",
    "\n",
    "# Run queries with optimized prompts\n",
    "response = query_engine.query(\"What security measures does your company implement?\")\n",
    "\"\"\"\n",
    "\n",
    "md(f\"```python\\n{complete_integration}\\n```\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Conclusions\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Significant Token Savings**: ScaleDown integration reduced prompt tokens by approximately {avg_token_savings:.1f}%, leading to cost savings and reduced latency.\n",
    "\n",
    "2. **Response Quality Maintained**: The optimized prompts generated responses of comparable quality to the baseline.\n",
    "\n",
    "3. **Easy Integration**: ScaleDown can be integrated at various points in the LlamaIndex pipeline with minimal code changes.\n",
    "\n",
    "4. **Environmental Impact**: By tracking carbon emissions, we can quantify the environmental benefits of prompt optimization.\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- **Fine-tune optimization rates** for different types of queries\n",
    "- **Implement custom prompt templates** optimized with ScaleDown\n",
    "- **Create a custom query engine** that integrates deeply with ScaleDown's API\n",
    "\n",
    "ScaleDown provides a powerful way to enhance your LlamaIndex RAG pipelines by reducing token usage while maintaining performance, making your AI applications more cost-effective and environmentally friendly.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
